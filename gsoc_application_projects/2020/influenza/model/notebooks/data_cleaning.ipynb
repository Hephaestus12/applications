{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "We will be cleaning the data to be eventually used to train the model to predict influenza cases.\n",
    "\n",
    "We start off with importing all the libraries we are going to use for this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from config import COUNTRIES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We set up our data paths. We had already combined the data from the several different csv files into one a single one using the `combine.py` file in the `src` module.\n",
    "\n",
    "These files are named after their countries and present in the data/combined directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "path = Path.cwd()\n",
    "data_path = path.parent / 'data' / 'combined'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Making Dataframes from the .csv files\n",
    "\n",
    "`df`, here, contrary to conventional use, doesn't refer to the name of a pandas dataframe. Instead it is a dictionary of pandas dataframes. The key is the country name and the value is the dataframe corresponding to that country."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "df = {}\n",
    "for country in COUNTRIES :\n",
    "    file_path = data_path / (country+'.csv')\n",
    "    df[country] = pd.read_csv(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning data for Austria\n",
    "We will start off with cleaning the data for just one country. If you notice, all the data in all the files is pretty similar. It is after all the same data, but for different countries. What we deduce from the dataframe for *austria*, we shall implement for all other countries.\n",
    "\n",
    "Just to get an idea about what is this data we are  dealing with:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "       \"RTS,S\"  Acetaldehydsyndrom  Achsensymptom  Adeno-assoziierte_Viren  \\\ncount    156.0          156.000000     156.000000               156.000000   \nmean       0.0          106.044872       2.307692               114.685897   \nstd        0.0           35.336419       2.699141                29.542709   \nmin        0.0           48.000000       0.000000                31.000000   \n25%        0.0           81.750000       0.000000                97.000000   \n50%        0.0           96.000000       2.000000               111.000000   \n75%        0.0          130.250000       3.000000               132.000000   \nmax        0.0          216.000000      12.000000               193.000000   \n\n       Adenovirusimpfstoff     Adipsie    Adynamie  Aggravation  \\\ncount           156.000000  156.000000  156.000000   144.000000   \nmean              4.512821  107.423077  242.121795   404.152778   \nstd               5.373156   62.227578   96.730777    82.270177   \nmin               0.000000   26.000000   79.000000   140.000000   \n25%               0.000000   60.000000  173.000000   365.250000   \n50%               1.000000   80.000000  216.500000   399.500000   \n75%               8.000000  164.500000  289.750000   450.000000   \nmax              21.000000  264.000000  511.000000   651.000000   \n\n       Agitation_(Medizin)  Akantholyse     ...       Zervizitis  \\\ncount                156.0   156.000000     ...       156.000000   \nmean                   0.0   111.160256     ...        23.044872   \nstd                    0.0    43.810663     ...        33.477646   \nmin                    0.0    33.000000     ...         0.000000   \n25%                    0.0    85.000000     ...         0.000000   \n50%                    0.0    99.000000     ...         1.000000   \n75%                    0.0   131.000000     ...        55.250000   \nmax                    0.0   334.000000     ...       115.000000   \n\n       Zikavirus-Epidemie_2015/2016  Zohlen-Zeichen      Zoonose  \\\ncount                    156.000000      156.000000    66.000000   \nmean                      21.217949      185.237179  1330.560606   \nstd                       32.972578       85.005891   360.611068   \nmin                        0.000000       45.000000   587.000000   \n25%                        0.000000      127.750000  1074.250000   \n50%                        0.000000      158.000000  1356.000000   \n75%                       45.750000      230.000000  1508.750000   \nmax                      150.000000      492.000000  2980.000000   \n\n       Zwerchfellhochstand      Zyanose       cases    incidence         Ödem  \\\ncount           156.000000   156.000000  130.000000   156.000000   156.000000   \nmean            262.717949  1933.211538  398.892308   880.461157  1579.358974   \nstd             112.797781   588.776446  239.507934   429.093467  2262.157075   \nmin              95.000000   691.000000    0.000000     0.000000     0.000000   \n25%             180.500000  1461.250000  258.000000   636.664425     0.000000   \n50%             220.000000  1871.500000  380.000000   816.951024     7.000000   \n75%             345.750000  2342.250000  532.000000  1120.661459  4296.250000   \nmax             568.000000  3600.000000  992.000000  2052.819744  5756.000000   \n\n          Übelkeit  \ncount   156.000000  \nmean    251.942308  \nstd     360.486486  \nmin       0.000000  \n25%       0.000000  \n50%       3.000000  \n75%     665.250000  \nmax    1004.000000  \n\n[8 rows x 383 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>\"RTS,S\"</th>\n      <th>Acetaldehydsyndrom</th>\n      <th>Achsensymptom</th>\n      <th>Adeno-assoziierte_Viren</th>\n      <th>Adenovirusimpfstoff</th>\n      <th>Adipsie</th>\n      <th>Adynamie</th>\n      <th>Aggravation</th>\n      <th>Agitation_(Medizin)</th>\n      <th>Akantholyse</th>\n      <th>...</th>\n      <th>Zervizitis</th>\n      <th>Zikavirus-Epidemie_2015/2016</th>\n      <th>Zohlen-Zeichen</th>\n      <th>Zoonose</th>\n      <th>Zwerchfellhochstand</th>\n      <th>Zyanose</th>\n      <th>cases</th>\n      <th>incidence</th>\n      <th>Ödem</th>\n      <th>Übelkeit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>156.0</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>144.000000</td>\n      <td>156.0</td>\n      <td>156.000000</td>\n      <td>...</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>66.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>130.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n      <td>156.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.0</td>\n      <td>106.044872</td>\n      <td>2.307692</td>\n      <td>114.685897</td>\n      <td>4.512821</td>\n      <td>107.423077</td>\n      <td>242.121795</td>\n      <td>404.152778</td>\n      <td>0.0</td>\n      <td>111.160256</td>\n      <td>...</td>\n      <td>23.044872</td>\n      <td>21.217949</td>\n      <td>185.237179</td>\n      <td>1330.560606</td>\n      <td>262.717949</td>\n      <td>1933.211538</td>\n      <td>398.892308</td>\n      <td>880.461157</td>\n      <td>1579.358974</td>\n      <td>251.942308</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>35.336419</td>\n      <td>2.699141</td>\n      <td>29.542709</td>\n      <td>5.373156</td>\n      <td>62.227578</td>\n      <td>96.730777</td>\n      <td>82.270177</td>\n      <td>0.0</td>\n      <td>43.810663</td>\n      <td>...</td>\n      <td>33.477646</td>\n      <td>32.972578</td>\n      <td>85.005891</td>\n      <td>360.611068</td>\n      <td>112.797781</td>\n      <td>588.776446</td>\n      <td>239.507934</td>\n      <td>429.093467</td>\n      <td>2262.157075</td>\n      <td>360.486486</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.0</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n      <td>0.000000</td>\n      <td>26.000000</td>\n      <td>79.000000</td>\n      <td>140.000000</td>\n      <td>0.0</td>\n      <td>33.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>45.000000</td>\n      <td>587.000000</td>\n      <td>95.000000</td>\n      <td>691.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.0</td>\n      <td>81.750000</td>\n      <td>0.000000</td>\n      <td>97.000000</td>\n      <td>0.000000</td>\n      <td>60.000000</td>\n      <td>173.000000</td>\n      <td>365.250000</td>\n      <td>0.0</td>\n      <td>85.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>127.750000</td>\n      <td>1074.250000</td>\n      <td>180.500000</td>\n      <td>1461.250000</td>\n      <td>258.000000</td>\n      <td>636.664425</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.0</td>\n      <td>96.000000</td>\n      <td>2.000000</td>\n      <td>111.000000</td>\n      <td>1.000000</td>\n      <td>80.000000</td>\n      <td>216.500000</td>\n      <td>399.500000</td>\n      <td>0.0</td>\n      <td>99.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>158.000000</td>\n      <td>1356.000000</td>\n      <td>220.000000</td>\n      <td>1871.500000</td>\n      <td>380.000000</td>\n      <td>816.951024</td>\n      <td>7.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.0</td>\n      <td>130.250000</td>\n      <td>3.000000</td>\n      <td>132.000000</td>\n      <td>8.000000</td>\n      <td>164.500000</td>\n      <td>289.750000</td>\n      <td>450.000000</td>\n      <td>0.0</td>\n      <td>131.000000</td>\n      <td>...</td>\n      <td>55.250000</td>\n      <td>45.750000</td>\n      <td>230.000000</td>\n      <td>1508.750000</td>\n      <td>345.750000</td>\n      <td>2342.250000</td>\n      <td>532.000000</td>\n      <td>1120.661459</td>\n      <td>4296.250000</td>\n      <td>665.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.0</td>\n      <td>216.000000</td>\n      <td>12.000000</td>\n      <td>193.000000</td>\n      <td>21.000000</td>\n      <td>264.000000</td>\n      <td>511.000000</td>\n      <td>651.000000</td>\n      <td>0.0</td>\n      <td>334.000000</td>\n      <td>...</td>\n      <td>115.000000</td>\n      <td>150.000000</td>\n      <td>492.000000</td>\n      <td>2980.000000</td>\n      <td>568.000000</td>\n      <td>3600.000000</td>\n      <td>992.000000</td>\n      <td>2052.819744</td>\n      <td>5756.000000</td>\n      <td>1004.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 383 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['austria'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Columns: 385 entries, \"RTS,S\" to date\n",
      "dtypes: float64(383), object(2)\n",
      "memory usage: 469.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df['austria'].info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dealing with missing values\n",
    "\n",
    "The first instinct when dealing with missing data is to try and see if there is any easy way out. We will start of by dropping all rows where any element is zero:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "(0, 385)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['austria'].dropna().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, doing this reduced our dataset to zero rows. Every row has some or the other null value.\n",
    "\n",
    "We can now try a column specific approaach. i.e. dropping all columns with even one single null value:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "(156, 311)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['austria'].dropna(axis=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of columns reduced from 383 to 311 , that is a little over 80%. This is a good enough number of non-null features.\n",
    "\n",
    "However, lets try to see if we can increase this number. We shall proceed to check how many values are null in each of those 72 columns:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "Otitis_externa    156\nSyndrom           156\nEpidemie          155\nVSV-EBOV          154\nBartholinitis     150\ndtype: int64"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = df['austria'].isnull().sum().sort_values(ascending=False)\n",
    "total.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You know what would be more helpfull that this? If we could get a percentage estimation of this. as in, instead of how many rows have null, how much percentage of rows have null values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def find_null_percent(data):\n",
    "    percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "    return percent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "Otitis_externa    1.000000\nSyndrom           1.000000\nEpidemie          0.993590\nVSV-EBOV          0.987179\nBartholinitis     0.961538\ndtype: float64"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_null_percent(df['austria']).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As far as those column are concerned where the amount of data missing is more than 20%(0.2), if we try to infer the data, it will most probably be wrong and can misguide our training model.\n",
    "\n",
    "We drop those columns from our dataset where the number of null values is more than 20%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "null_percent_list = find_null_percent(df['austria'])\n",
    "for column, null_percent in null_percent_list.items() :\n",
    "    if null_percent > 0.2:\n",
    "        df['austria'] = df['austria'].drop(columns=[column])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what the damage is:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "(156, 316)"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['austria'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "Parotitis                 0.192308\ncases                     0.166667\nRhinosinusitis            0.153846\nFieber                    0.108974\nAggravation               0.076923\ndate                      0.000000\nHospitalfieber            0.000000\nHerdenzephalitis          0.000000\nHexavalenter_Impfstoff    0.000000\nHexenschuss               0.000000\ndtype: float64"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_null_percent(df['austria']).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only 5 columns having any null values have null values less than 20%. Let's see if we can try to infer this data from the rest of the data.\n",
    "\n",
    "Also, the columns called *cases* isn't really a list of wikipedia pageviews for any article. So we get rid of that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "df['austria'] = df['austria'].drop(columns='cases')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have to deal with these 4 columns.\n",
    "Since this data is similar to a time series, we can use methods used to there to infer the mssing data.\n",
    "\n",
    "Namely linear interpollation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "df['austria'] = df['austria'].interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "df['austria'] = df['austria'].interpolate(method='linear', limit_direction='backward', axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have successfully gotten rid of all null values in this dataframe. Let's verify this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "date                      0.0\nHypalbuminämie            0.0\nHerdenzephalitis          0.0\nHexavalenter_Impfstoff    0.0\nHexenschuss               0.0\ndtype: float64"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_null_percent(df['austria']).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have to do what we have done here to all the countries.\n",
    "\n",
    "We will do this via a simple for loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For belgium\n",
      "\tnumber of columns before cleaning is 385\n",
      "\tnumber of columns after cleaning is 367\n",
      "For germany\n",
      "\tnumber of columns before cleaning is 384\n",
      "\tnumber of columns after cleaning is 372\n",
      "For italy\n",
      "\tnumber of columns before cleaning is 473\n",
      "\tnumber of columns after cleaning is 473\n",
      "For netherlands\n",
      "\tnumber of columns before cleaning is 385\n",
      "\tnumber of columns after cleaning is 367\n"
     ]
    }
   ],
   "source": [
    "for country in COUNTRIES:\n",
    "        if country == 'austria':\n",
    "            continue\n",
    "        print('For ' + country)\n",
    "        print('\\tnumber of columns before cleaning is '+ str(df[country].shape[1]))\n",
    "\n",
    "        if 'cases' in df[country].columns:\n",
    "            df_country = df[country].drop(columns=['cases'])\n",
    "\n",
    "        # drop columns with all more than 20% data missing\n",
    "        percent_missing = find_null_percent(df['austria'])\n",
    "\n",
    "        for column, null_percent in percent_missing.items():\n",
    "            if null_percent > 0.2:\n",
    "                df[country] = df[country].drop(columns=[column])\n",
    "\n",
    "        # perform linear interpolation on remaining missing values\n",
    "        df[country] = df[country].interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "        df[country] = df[country].interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "\n",
    "        # drop columns with all values as zeroes\n",
    "        df[country] = df[country].loc[:, (df[country] != 0).any(axis=0)]\n",
    "\n",
    "        print('\\tnumber of columns after cleaning is '+ str(df[country].shape[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have successfully cleaned all country dataframes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}